# Flappy Bird (Simplified) â€” DQN Agent

A minimal reinforcement learning project where a Deep Q-Network (DQN) learns to play a **simplified Flappy Bird** environment. The env is lightweight (no graphics) and uses shaped rewards so the agent can learn to survive and pass pipes.

<p align="center">
  <em>Train on CPU in minutes; extend to Double/Dueling/Per later.</em>
</p>

---

## âœ¨ Features

* Tiny, self-contained environment (no Gym required)
* DQN with target network & replay buffer
* Reward shaping for faster learning (survival + gap-centering + pipe pass, death penalty)
* Easy hyperparameters you can tune from CLI
* Save/Load model for evaluation

---

## ğŸ“¦ Project Structure

```
flappy-bird-dqn/
â”œâ”€â”€ flappy_dqn.py         
â”œâ”€â”€ requirements.txt      
â”œâ”€â”€ .gitignore            
â””â”€â”€ README.md              
```

> If you only want the bare minimum, keep just `flappy_dqn.py` and `requirements.txt`.

---

## ğŸ§  Algorithm

This project implements a vanilla **DQN**:

* Q-network: 2 hidden layers (default 64â†’64; recommended 128â†’128)
* Target network for stable bootstrapping
* Replay buffer with uniform sampling
* Îµ-greedy exploration with slow decay

**Reward shaping** (recommended default):

* `+0.1` per step survived
* `+5.0` when a pipe is passed
* `-5.0` on death
* *(Optional, stronger shaping)*: small penalty for distance from the gap center each step

---

## ğŸš€ Quickstart

### 1) Install

```bash
pip install -r requirements.txt
```

<details>
<summary>requirements.txt</summary>

```
numpy
torch>=2.0.0
pygame>=2.5.0
```

</details>

### 2) Train
```bash
python flappy_dqn.py --episodes 2000
```

During training, youâ€™ll see logs like:

```
Ep 123, Score 2, Reward 7.40, Avg20 1.25, Eps 0.742
```

* **Score**: pipes passed in that episode
* **Reward**: sum of shaped rewards
* **Avg20**: moving average of last 20 episode scores
* **Eps**: current Îµ for Îµ-greedy

### 3) Evaluate

```bash
python flappy_dqn.py --eval --model flappy_dqn.pt
```

* Loads the saved model and runs greedy play (Îµ=0)

---

## âš™ï¸ CLI Options

```
--episodes INT        # number of training episodes (default 2000)
--eval                # run evaluation instead of training
--model PATH          # model path for save/load (default flappy_dqn.pt)
```

---

## ğŸ”§ Tuning Tips (to beat >3 pipes)

1. **Network size**: bump to `128â†’128` (or `256â†’256`) in `DQN`.
2. **Exploration**: slow decay `eps_decay=0.999`, set floor `eps_min=0.1`.
3. **Extra shaping**: penalize distance to gap center each step:

   ```python
   # inside env.step()
   reward -= abs(self.bird_y - self.pipe_gap_y) * 0.05
   ```
4. **Replay buffer**: increase to `200_000` to diversify experiences.
5. **More training**: try 5kâ€“10k episodes; Flappy Bird is sparse & timing-heavy.
6. **Advanced**: implement **Double DQN**, **Dueling**, **PER** for bigger gains.

---

## ğŸ§© Minimal Code Snippet (Env + Reward Shaping)

> Full code lives in `flappy_dqn.py`; this snippet shows the key reward changes.

```python
# reward shaping inside step()
reward = 0.1  # survival
passed_pipe = False

if self.pipe_x < 0:
    self.pipe_x = 1.0
    self.pipe_gap_y = np.random.uniform(0.3, 0.7)
    passed_pipe = True
    reward += 5.0  # pipe passed

# (optional) encourage staying near gap center
reward -= abs(self.bird_y - self.pipe_gap_y) * 0.05

if collision:
    reward -= 5.0  # death penalty
```

---

## ğŸ§ª Expected Results

* With the defaults, expect average score around **1â€“3 pipes** within \~2k episodes.
* With tuning above (bigger net + slower Îµ decay + distance penalty), average can push further.

> For reproducible comparisons, fix a random seed and report Avg20.

---

## ğŸ›  Troubleshooting

* **`ModuleNotFoundError: numpy/torch`** â†’ run `pip install -r requirements.txt` inside the same environment you call `python` from. Verify with `which python` and `which pip`.
* **Very low scores** â†’ increase episodes, slow Îµ decay, use 128â†’128 hidden units, add distance-to-gap penalty.
* **Torch install slow** â†’ CPU wheels install quickly; for CUDA use PyTorchâ€™s extra index URL matching your driver.

---

## ğŸ¤ Contributing

* Issues and PRs welcome! Please include: env details, command used, and a short log snippet.

---

## ğŸ“£ Citation

If this repo helps your work or teaching, consider citing it:

```
@misc{flappy_dqn_minimal,
  title  = {Flappy Bird (Simplified) â€” DQN Agent},
  author = {Your Name},
  year   = {2025},
  note   = {Minimal no-graphics environment with reward shaping}
}
```

---

### â­ If you publish this on GitHub

1. Create a new repo and add these files.
2. Paste this README as `README.md`.
3. Commit `flappy_dqn.py` and `requirements.txt`.
4. (Optional) Add `LICENSE` and `.gitignore` from above.
5. Push and share! ğŸš€
